# Research and Industry Reports 

### International research reports:
**International Energy Agency report on Energy & AI** (2025): [Link](https://www.iea.org/reports/energy-and-ai)

- The International Energy Agency's (IEA) report on "Energy and AI" explores the intersection of artificial intelligence and the energy sector. It highlights how AI's growing electricity demand could impact energy security, emissions, innovation, and affordability. The report provides projections for AI's electricity consumption over the next decade and identifies key energy sources to meet this demand. It also emphasizes the transformative potential of AI in optimizing energy systems and driving innovation, while addressing the challenges policymakers face due to the lack of comprehensive data.

- Three highlights:
  - Electricity Consumption: Data centers are projected to consume up to 20% of global electricity by 2035 due to the increasing demand for AI and cloud services.
  - Efficiency Gains: AI-driven optimization can reduce data center energy consumption by up to 40%, significantly lowering operational costs and environmental impact.
  - Renewable Energy Integration: By 2030, over 50% of data centers are expected to be powered by renewable energy sources, contributing to a substantial reduction in carbon emissions.

### Researcher & University reports: 
**Cornell University paper on Green Prompting** (2025): [Link](https://arxiv.org/abs/2503.10666) 

- The report investigates the energy consumption of Large Language Models (LLMs) during inference, focusing on how different prompt and response characteristics impact energy usage. The study reveals that the length of the response generated by LLMs is a major driver of energy consumption, more so than the length of the prompt itself. It also identifies specific keywords that correlate with higher or lower energy usage, highlighting the importance of prompt design in optimizing inference efficiency. The findings suggest that the semantic meaning of prompts significantly influences energy costs, paving the way for more sustainable AI practices by creating energy-adaptive LLMs.

